\documentclass[titlepage]{tufte-book}

\usepackage[runall=true]{pythontex}
\setpythontexworkingdir{<outputdir>}
\usepackage{environ}
\usepackage{morewrites}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{listings}
\usepackage{array}
\usepackage{extarrows}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usepackage{hyperref}
\usepackage{graphviz}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\usepackage{bashful}
\usepackage{microtype} % Improves character and word spacing
\usepackage{caption}

\usepackage{booktabs} % Better horizontal rules in tables

\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio} % Improves figure scaling
\graphicspath{{figures/}}

\usepackage{fancyvrb} % Allows customization of verbatim environments
\fvset{fontsize=\normalsize} % The font size of all verbatim text can be changed here

\newcounter{problem}
\newcounter{total}
\newcommand{\step}[1]{{}
\vspace{4pt} \noindent {\bf \theproblem. }#1\addtocounter{problem}{1}}

\newcommand{\cut}[1]{}

\usepackage[tikz]{bclogo}
\usepackage{tikz}
\usetikzlibrary{calc}

\lstdefinestyle{BashInputStyle}{
  language=bash,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny,
  showstringspaces=false,
  numbersep=3pt,
  otherkeywords={|, ;, ', ", *,>, <, *, &, `, $},
  alsoletter={:~_},
  columns=fullflexible,
  backgroundcolor=\color{yellow!20},
  linewidth=0.93\linewidth,
  xleftmargin=0.03\linewidth,
  keywordstyle=\color{blue},
  emph={ls, cat, head, tail, more, less, sort, uniq, kill java, grep, zip, unzip, tar, wc, cp, chmod,chown},
  emphstyle=\color{black}\bfseries,
    commentstyle=\color{gray}\slshape
    }

\newcommand{\chili}{\scalebox{.04}{\includegraphics{figures/chili.pdf}}}
\newcommand{\chchili}{{\chili\chili}}
\newcommand{\chchchili}{{\chchili\chili}}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

\hypersetup{
urlcolor=blue,
colorlinks=true
}
\usepackage[noline, procnumbered, linesnumberedhidden, boxed]{algorithm2e}

\newcommand{\openepigraph}[2]{ % This block sets up a command for printing an epigraph with 2 arguments - the quote and the author
\begin{fullwidth}
\sffamily\large
\begin{doublespace}
\noindent\allcaps{#1}\\ % The quote
\noindent\allcaps{#2} % The author
\end{doublespace}
\end{fullwidth}
}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\renewcommand{\thefigure}{\arabic{figure}}

\newenvironment{callout}[1]{
\[
  \left[
      \begin{tabular}{@{\quad}m{.05\textwidth}@{\qquad}m{.75\textwidth}@{\quad}}
        \scalebox{1.5}{#1} & 
          \raggedright%
}
{
      \end{tabular}
    \right]
\]
}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage} % Command to insert a blank page

\usepackage{makeidx} % Used to generate the index
\makeindex % Generate the index which is printed at the end of the document

\renewcommand{\maketitlepage}[0]{%
  \cleardoublepage%
  {%
  \sffamily%
  \begin{fullwidth}%
  ~
  \vspace{11.5pc}%
  \fontsize{36}{40}\selectfont\par\noindent\textcolor{darkgray}{\allcaps{\thanklesstitle}}%
  
\scalebox{.2}{\includegraphics{figures/msan-logo}}
  \vspace{11.5pc}%
  \fontsize{12}{18}\selectfont\par\indent\textcolor{darkgray}{\allcaps{\thanklessauthor}\\
\indent{\tt parrt@cs.usfca.edu}\\
\href{http://parrt.cs.usfca.edu}{http://parrt.cs.usfca.edu}}%
  \vspace{11.5pc}%
  \fontsize{14}{16}\selectfont\par\noindent\allcaps{\thanklesspublisher}%
  \end{fullwidth}%
  }
  \thispagestyle{empty}%
  \clearpage%
}

\titlecontents{part}% FIXME
    [0em] % distance from left margin
    {\vspace{1.5\baselineskip}\begin{fullwidth}\LARGE\rmfamily\itshape} % above (global formatting of entry)
    {\contentslabel{2em}} % before w/label (label = ``II'')
    {} % before w/o label
    {\rmfamily\upshape\qquad\thecontentspage} % filler + page (leaders and page num)
    [\end{fullwidth}] % after

  \titlecontents{chapter}%
    [0em] % distance from left margin
    {\vspace{1.5\baselineskip}\begin{fullwidth}\Large\rmfamily\itshape} % above (global formatting of entry)
    {\hspace*{0em}\contentslabel{2em}} % before w/label (label = ``2'')
    {\hspace*{4em}} % before w/o label
    {\rmfamily\upshape\qquad\thecontentspage} % filler + page (leaders and page num)
    [\end{fullwidth}] % after

\titlespacing*{\chapter}{0pt}{0pt}{30pt}
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus.2ex}

\begin{document}

\chapter{Confidence Intervals for Price of Hostess Twinkies}

\setcounter{problem}{1}
\section{Goal}

\begin{fullwidth}

The goal of this project is to learn how to compute an empirical 95\% confidence interval for the sample mean price for Hostess Twinkies using an awesome technique called {\em bootstrapping}. Bootstrapping allows us to assess the accuracy of a sample mean we've computed, giving us data about how sample means vary. As part of this lab, you will also learn to read in a file full of numbers. In this case, we are going to read in the price of Hostess Twinkies, a tasty snack recently returned from the dead, from around the US.

~\\
\noindent Please do your work in filename {\em userid}{\tt -conf/bootstrap.py}.

\section{Discussion}

Given a list of prices for Twinkies, we can easily compute the mean (average). More specifically, we'd be computing the {\em sample mean}, which is not usually exactly the same as the {\em population mean}. Getting the population mean is either impractical (pollsters cannot poll every single voter) or impossible and so we must be satisfied with the sample mean.  Unfortunately, the sample mean varies from sample to sample so it's more informative to use an interval statistic rather than a point statistic. The interval describes the uncertainty of the sample mean as an estimate of the true population mean.  A sample mean confidence interval of 95\% tells us the range in which most of the sample means should fall. Therefore, a confidence interval gives us an estimated range that is likely to include the population mean.

Given just one set of numbers, such as our Twinkie prices, (just one mean) how do we get an interval? Using the central limit theorem, which tells us how sample means, $m$, bounce around the true population mean, $\mu$: sample mean $m$ is drawn from $N(\mu, \sigma^2/n)$ for sample size $n$.  Because we don't know the population statistics, we use the sample mean, $\overline{X}$, and sample variance, $s^2$, as population estimates. Then, we look at the range of 95\% of the mass under the sample mean distribution, $N(\mu, \sigma^2/n)$.  The range is $1.96s$ to the left and right of the sample mean $m$: 
\begin{center}
\scalebox{.4}{\includegraphics{figures/clt.pdf}}
\end{center}

But, is that the best we can do?  Here's the problem: For each sample we collect, we will get a different sample mean and variance which means that our confidence interval is also a random variable that bounces around. The distribution would appear to shift around if we were plotting the sample mean distributions as the samples were collected. The edges of our interval would be look fuzzy. Dang!

We've seen how increasing the number of samples (not sample size) increases precision or resolution when visualizing distributions for the central limit theorem. We can't increase the sample size to get a narrower interval, but we {\em can} improve the precision of our interval estimate by acquiring more samples from whatever the Twinkies distribution happens to be.

How do we get lots of samples from an underlying distribution that we cannot identify? Through the magic of {\em bootstrapping} where we repeatedly {\em resample} from our single sample {\em with replacement}.  To get a new sample, we randomly select $n$ values from our known data set of size $n$, leaving each value available to be selected again. That gives us one trial and we compute our test statistic, the mean, on each sample $X$.  For $M$ such trials, we get $M$ sample means, $\bar{X}$.  To get the 95\% interval for the mean, we sort the means and strip away the bottom 2.5\% and the top 2.5\% of the means. For example, if $M$ is 200, we drop the smallest 5 ($200 \times 0.025$) and largest 5 means from the list of 200 means. Then, the lowest and highest value in that truncated list represent the boundaries of the confidence interval. Cool, right?

\begin{callout}{\bcinfo}
The histogram of the sample means from all trials would approximate a normal distribution, but the amazing thing is bootstrapping works even when the central limit theorem does not apply! I.e., when we don't know what the point statistic distribution is. That is the power of simulation.
\end{callout}

To verify that we are doing the right thing, we will draw the theoretical normal distribution expected by the central limit theorem and then shade in the 95\% theoretical confidence interval, which we know is 1.96 standard deviations on either side of the mean: $\mu \pm 1.96\sigma$. NOTE: Because we have to use the sample estimates for $\mu$ and $\sigma$, we will be more sure about our bootstrapped estimates than this visualization. The visualization will be just for sanity checking.

\section{Steps}

\step First, we have to get the data from a file called {\tt prices.txt} from \\
{\tt\small https://github.com/parrt/msan501/tree/master/data}:

\begin{pyverbatim}
prices = []
f = open("prices.txt")
for line in f:
	v = float(line.strip())
	prices.append(v)
\end{pyverbatim}

When debugging or during development, you can print those numbers out to verify they look okay.

\step Now, we need a function that lets us sample {\em with replacement} from that raw data set. In other words, we need a function that gets $n$ values at random from a data parameter (a list of numbers). It should allow repeated grabbing of the same value (that's what we call ``with replacement'').

\begin{pyverbatim}
def sample(data):
	"""
	Return a random sample of data values with replacement.
	The returned array has same length as data.
	"""
\end{pyverbatim}

The idea is to get an array of random numbers from $U(0,n-1)$ for {\tt n=len(data)}. You can use Python's built-in random number generator ({\tt import random}) or your own from our previous lecture but make sure to add it to your repository.  These random values are a set of indices into the data array so just loop through this index array grabbing values from {\tt data} according to the index. For example if you have indexes = [3,9] for a 2-element {\tt data} array, then return a new array {\tt [data[3], data[9]]}. My solution for this  function is 2 lines.

\step Now define TRIALS=40 and perform that many samplings of prices. For each sample, create the sample mean and add it to a list variable called {\tt X\_}.

\step Sort that list and get the values at indices TRIALS*0.025 and TRIALS*0.975 in {\tt X\_}.

\step Print these left and right values as that will tell you the bounds of your 95\% confidence interval. You might get something like (there will be a lot of variation) 1.13057101449 and 1.1880057971.

\step Add matplotlib boilerplate code to create a figure and to plot diamonds on the graph at those locations.  I use {\tt ax} variable below; here's how I get it:
\begin{pyverbatim}
...
fig = plt.figure()
ax = fig.add_subplot(111)
...
\end{pyverbatim}
\step Now plot the normal curve using your amazing understanding of the central limit  theorem. Use the following range and also set the overall graph range:

\begin{pyverbatim}
import scipy.stats as stats # you might have to install scipy with pip

x = np.arange(1.05, 1.25, 0.001)
plt.axis([1.10, 1.201, 0, 30])
x = np.arange(1.05, 1.25, 0.001)
y = stats.norm.pdf(x,mean,stddev)  # WHAT ARE MEAN AND STDDEV?
plt.plot(x,y, color='red')
\end{pyverbatim}

\step Run it and you should get the following graph:

\scalebox{.4}{\includegraphics{figures/conf-40-basic.pdf}}

Ok, that's great but we have no idea if this is correct or not. Now, let's go nuts and show lots of stuff on the graph.

\step First, let's shade in the theoretical 95\% confidence interval using a function called {\tt normpdf()} that you implement from the symbolic definition of the normal density function:

\[
normpdf(x) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}
\]


\begin{pyverbatim}
def normpdf(x, mu, sigma):
    return ...

mean = ...
stddev = ...
# redraw normal but only shade in 95% CI
clt_left  = ...
clt_right = ...

ci_x = np.arange(clt_left, clt_right, 0.001)
ci_y = normpdf(ci_x,mean,stddev)
# shade under (ci_x,ci_y) curve
plt.fill_between(ci_x,ci_y,color="#F8ECE0") 
\end{pyverbatim}

\noindent Run it again to see how it shades in the graph.

\scalebox{.4}{\includegraphics{figures/conf-40-noannot.pdf}}

\step Now let's annotate with lots of information. Please read through and figure out what all of that stuff does to draw the nice arrows and so on.

{\small
\begin{pyverbatim}
plt.text(.02,.95, '$TRIALS = %d$' % TRIALS, transform = ax.transAxes)
plt.text(.02,.9,  '$mean(prices)$ = %f' % np.mean(prices), transform = ax.transAxes)
plt.text(.02,.85, '$mean(\\overline{X})$ = %f' % np.mean(X_), transform = ax.transAxes)
plt.text(.02,.80, '$stddev(\\overline{X})$ = %f' %
    np.std(X_,ddof=1), transform = ax.transAxes)
plt.text(.02,.75, '95%% CI = $%1.2f \\pm 1.96*%1.3f$' %
   (np.mean(X_),np.std(X_,ddof=1)), transform = ax.transAxes)
plt.text(.02,.70, '95%% CI = ($%1.2f,\\ %1.2f$)' %
				  (np.mean(X_)-1.96*np.std(X_),
				   np.mean(X_)+1.96*np.std(X_)),
		 transform = ax.transAxes)

plt.text(1.135, 11.5, "Expected", fontsize=16)
plt.text(1.135, 10, "95% CI $\\mu \\pm 1.96\\sigma$", fontsize=16)
plt.title("95% Confidence Intervals: $\\mu \\pm 1.96\\sigma$", fontsize=16)

ax.annotate("Empirical 95% CI",
			 xy=(inside[0], .3),
			 xycoords="data",
			 xytext=(1.13,4), textcoords='data',
			 arrowprops=dict(arrowstyle="->",
                            connectionstyle="arc3"),
			 fontsize=16)
\end{pyverbatim}
}

\step Run it and you should get the following graph:

\scalebox{.4}{\includegraphics{figures/conf-40.pdf}}

\step We need to make the number of trials a parameter and let's do the annotations as well.  

\begin{pyverbatim}
import sys

fancy=False
TRIALS = 40
if len(sys.argv)>1:
    TRIALS = int(sys.argv[1])
    if len(sys.argv)>2 and sys.argv[2]=='-fancy':
        fancy=True
\end{pyverbatim}

\noindent Then wrap your annotation code in {\tt if fancy} and add code to save your graph (in all cases):

\begin{pyverbatim}
plt.savefig('bootstrap-'+str(TRIALS)+('-basic' if not fancy else '')+'.pdf', format="pdf")
\end{pyverbatim}

\step  We don't have to increase the number of trials very much before the confidence interval tightens up nicely.  500 trials should look something like:

\begin{alltt}
$ python bootstrap.py 500 -fancy
\end{alltt}

\scalebox{.4}{\includegraphics{figures/conf-500.pdf}}

\noindent If you run it multiple times, the edges of the interval should not bounce around very much where as they do with only 40 trials.

\begin{callout}{\bcplume}
{\bf Deliverables}. {\em userid}{\tt -conf/bootstrap.py} and a PDF called {\em userid}{\tt -conf/bootstrap-500.pdf}, the result of $TRIALS=500$.
\end{callout}

\end{fullwidth}

\end{document}