\chapter{Summarizing Reuters Articles with TFIDF}

\section{Goal}

\begin{fullwidth}

The goal of this task is to learn a core technique used in text analysis called {\em TFIDF} or {\em term frequency, inverse document frequency}.  We will use what is called a {\em bag-of-words} representation where the order of words in a document doesn't matter---we care only about the words and how often they are present. A word's TFIDF value is often used as a feature for document clustering or classification. We will use it simply as a document summary mechanism. The more a term helps to distinguish its enclosing document from other documents, the higher its TFIDF score.

This task is also an opportunity to practice organizing your Python code as a set of functions rather than an unstructured script (blob) with a bunch of global variables. You will also learn how to translate some simple algorithms written in pseudocode to Python code. As a practical matter, you will learn how to process XML files in Python.  Please use file {\tt tfidf/tfidf.py}.

\section{Discussion}

One way to summarize a text document is to list, say, the top 25 words that seem most important. That could also be used to compare documents to see if they're talking about the same thing. For example, I had to solve a problem 15 years ago to reduce noise in the forums of a Java developer's website.  Users were posting stupid posts about movies and were also putting database questions in the forum on GUIs. The goal was to detect non-Java posts and also to detect misplaced posts. What does it mean to ``talk about Java''?  How do I know when someone is talking about databases versus GUIs? My solution was to identify the words important to Java as a whole (``Java-speak''), database, and GUI posts.  Any posts that did not have words important to Java, were tossed out as irrelevant after giving them a mild smack on the snout. Similarly, posts without words relevant to databases were compared to vocabularies associated with other topics to see if another forum would be more appropriate. To make this work, I needed a precise definition of ``important words.'' As I did for that project, you will use a classic   text analysis technique called TFIDF in this project.

Certainly a word is important to a document if it's used a lot, but that would also include words like ``the'' so we need to discount words used frequently among our {\em corpus} (set of documents). So, we boost words used frequently in a document but attenuate words that are used  in a lot of documents.  For more on this topic, see \href{http://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html}{Introduction to Information Retrieval}.

The {\em term frequency} is just the term count within a document divided by the number of words in that document (some people use ``frequency'' to mean ``count'' but that is an affront to the gods):

\[\tag{Term frequency of term $t$, document $d$}
tf(t,d) = \frac{count(t), t \in d}{|d|}
\]

A term's {\em document frequency} is the count of documents containing that term divided by the total number of documents:

\[\tag{Document frequency of $t$ in $N$ documents}
df(t,N) = \frac{|\{d_i : t \in d_i, \ i = 1..N\}|}{N}
\]

\noindent We can think of the document frequency as the probability of seeing $t$ in a document.

In order to attenuate the TFIDF scores for terms with high document frequencies, we need the document frequency  in the denominator:

\[\tag{First approximation to TFIDF}
\text{\em tfidf}(t,d,N) = \frac{tf(t,d)}{df(t,N)}
\]

This formula is  meaningful but gives a poor term score because the document frequency tends to overwhelm the term frequency in the numerator so we take the log of the denominator first.  Here's the formula slightly rewritten as it is normally shown:

\[\tag{TFIDF with attenuated document frequency}
\text{\em tfidf}(t,d,N) = tf(t,d) \times log(\frac{1}{df(t,N)})
\]

\noindent When $t$ is in every document, $idf$ is $log(1/df(t,N)) = log(1) = 0$. When $t$ is in very few documents, such as $1/10^8$, $idf$ is $log(10^8)$, which is about 18.4.

\begin{center}
\fbox{\begin{minipage}{40em}
{\bf Aside.} To prevent division by 0 errors when a term does not exist in a corpus (e.g., $df(t,N)=0$ in search applications where we pass unknown term(s) $t$), we can simply add 1 to the denominator. This is similar to {\em additive smoothing} that you will see when estimating term probabilities in document classifiers. The technique is like pretending there is an imaginary document with every unknown word (and, indeed, every possible word). To keep document frequencies in $[0..1]$, we can to bump the document count, $N$, as well.

\[\tag{$df$ with smoothing}
df(t,N) = \frac{|\{d_i : t \in d_i, \ i = 1..N\}| + 1}{N+1}
\]

For example, if we have a vector of words in a search query $[the, apple, cat, foo]$ aggregated from 100 documents, we might get a set of document frequencies like this:

\[
[\frac{100}{100}, \frac{4}{100}, \frac{9}{100}, \frac{0}{100}]
\]

\noindent With smoothing, we would get:

\[
[\frac{101}{101}, \frac{5}{101}, \frac{10}{101}, \frac{1}{101}]
\]

\noindent This is like converting zeros to some really small number (assuming $N$ is large) and adding that same small number to the other document frequencies.
\end{minipage}}
\end{center}

To summarize a document, we can order its terms by \text{\em tfidf} in reverse order and look at the top 20 words, for example.  To get the lexicon of a topic like databases, we can collect a known set of database posts into a single document and compute the tfidf in association with the aggregated documents of the other topics. Any word below a certain threshold, that we find by eyeballing it, is considered not relevant to that particular topic.

For example, here is a set of terms and the associated \text{\em tfidf} computed from a sample Reuters article. It's clear that it's talking about Nielsen ratings for news programs, without even looking at the original article.

\begin{center}
\begin{tabular}{|c|c|}
\hline
Term & \text{\em tfidf}\\
\hline
    rating     & 0.12332931962551781\\
    fox     & 0.11911646171233138\\
    nbc     & 0.11911646171233138\\
    homes     & 0.11408838230482544\\
    cbs     & 0.0794109744748876\\
    audiences     & 0.0794109744748876\\
    neilsen     & 0.0794109744748876\\
    evening     & 0.06678324701893232\\
    abc     & 0.06678324701893232\\
    watching     & 0.0634765565309808\\
\hline
\end{tabular}
\end{center}
\label{default}

To compute TFIDF, we need an overall index that maps term $t$ to document frequency $df(t,N)$ for all $t$ in all $N$ documents and an index that maps document $d$ to another index that maps each $t \in d$ to $tf(t,d)$. From that, we can compute all of the TFIDF scores.  That is what you will do for this project, as described in the next section.

\section{Your task}

To implement this project, you have six key functions to implement.   Four of them are in the pseudocode shown in floating boxes interspersed below. You must translate them to Python in a file called {\tt tfidf.py}.

\begin{function}
\TitleOfAlgo{{\em words}(document $d$)}
\vspace{-4pt}
\KwIn{Document $d$}
\KwResult{non-unique list of words $wordlist$}
\Indp
 Replace numbers, punctuation, tab, carriage return, newline with space\\
 $wordlist$ = Split $d$ into words\\
 Strip out $w \in wordlist$ smaller than 3 letters\\
 Normalize $w \in wordlist$ to lowercase\\
\Return{wordlist}
\end{function}

\begin{function}
\TitleOfAlgo{{\em create\_indexes}(list of $files$)}
\vspace{-4pt}
\KwIn{List of filenames $files$}
\KwResult{(Map document name to Counter object that maps term to frequency map {\em tf\_map}, Counter object that maps term to document count $df$)}
\Indp
$df$ = Counter(); {\em tf\_map} = \{\}\\
\ForEach{$f$ in files}{
 $d$ = $get\_text(f)$\\
 $words$ = $words(d)$\\
 $n$ = $len(words)$\\
 $tf$ = Counter(words)\\
 \# walk unique word list\\
 \ForEach{$t \in tf$}{
 $tf[t]$ = $tf[t] / n$ \# convert to a term freq from count\\
 $df[t]$ += 1  ~~~~~\# not currently a frequency; it's a count\\
 }
 $tf\_map[f]$ = $tf$\\
}
\Return{(tf\_map,df)}
\end{function}

\begin{function}
\TitleOfAlgo{{\em doc\_tfidf}($tf, df, N$)}
\vspace{-4pt}
\KwIn{Term to frequency map $tf$}
\KwIn{Term to document count map $df$}
\KwIn{Number of documents $N$}
\KwResult{Map of each term in doc ($tf$) to TFIDF score}
\Indp
	$tfidf$ = \{\}\\
	\ForEach{$t \in tf$}{
		$df_t = df[t] / N$\\
		$idf_t = 1/df_t$ \\
		$tfidf[t] = tf[t] \times log(idf_t)$ \\
	}
	\Return{$tfidf$} \\
\end{function}

\begin{function}
\TitleOfAlgo{{\em create\_tfidf\_map}($files$)}
\vspace{-4pt}
\KwIn{List of xml filenames $files$}
\KwResult{Map from file name to map of term to TFIDF scores}
\Indp
	$(tf\_map, df) = create\_indexes(files)$\\
	{\em tfidf\_map} = \{\}\\
	\ForEach{$f \in files$}{
		$tfidf$ = $doc\_tfidf(tf\_map[f], df)$\\
		{\em tfidf\_map}$[f]$ = $tfidf$\\
	}
	\Return{tfidf\_map} \\
\end{function}

You must also provide a function called {\tt filelist(pathspec)} that returns a list of all file names that match {\tt pathspec} and that {\em  have non-zero file sizes}:

\begin{pyverbatim}
def filelist(pathspec):
    ...
    return files
\end{pyverbatim}
	
For example, I might pass in string {\tt\small ../data/reuters-vol1-disk1/*.xml}. Naturally, if this doesn't work, then the rest of the code will not work as it won't get the proper data.  That function should only consider the files in the specified directory, not subdirectories.  You will probably want to use Python function {\tt glob.glob()}.

To process XML files use {\tt ElementTree}; learn about it \href{http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/}{\textcolor{blue}here}.  Create a function called {\tt get\_text()} to open a file, load it as XML, find the title and text elements and return that combines text as a string. It's important that we all follow the same text normalization so that we get the same word list and hence TFIDF scores for comparison.

\begin{pyverbatim}
# http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/
# summary: use ElementTree, good tutorial

def get_text(fileName):
    """
    Read an xml file and return the text from <title> and <text>.
    Concatenate those two elements, putting a space in between so it doesn't
    form an incorrect compound word.    
    """
    ...
    return text
\end{pyverbatim}

As part of your development work you will use lots of maps that look like \{dog: 36, cat: 19, ...\}.   Those integers, such as term counts, are easy to compute yourself but Python has an object that is effectively a histogram called \href{https://docs.python.org/2/library/collections.html#collections.Counter}{{\tt Counter}}. For example, if you give it a list of words, it will return an object that maps terms to their count. When you print them out, it will do so in reverse order of term count, which is very handy for testing.  Further, the unit tests I provide expect Counter objects.

For what it's worth, my implementation is just 60 lines including the {\tt import} statements. This is not a huge project but it is tricky when messing around with all of these maps of maps and lists of things. Start by understanding the problem and working a few TFIDF examples manually. Then, build a simple functions and test them individually before moving on to the more complex functions. For example, you should start by building {\tt filelist()} and then probably {\tt get\_text()}. My typical strategy is to design from the top down and test from the bottom up.

\subsection{XML Input}

As part of this project, I will provide you with a set of Reuters news articles in XML format, which will be the input to your program. From it, you will create the appropriate indexes and I will test those values against what I computed with my solution.

The format of the files doesn't matter much except that you need to pull out the {\tt title} and {\tt text} tags. The {\tt p} paragraph tags inside {\tt text} need to be collected. All of this text is what you will return from {\tt get\_text()}.
 
\begin{alltt}
<?xml version="1.0" encoding="iso-8859-1" ?>
<newsitem itemid="131701" ...>
<title>German consumer confidence rises in Aug/Sept</title>
<text>
<p>German consumer confidence rose...</p>
<p>The Icon index, which...</p>
</text>
...
</newsitem>
\end{alltt}

{\bf The collection of Reuters articles is considered proprietary to Reuters and, to get access to the data, the faculty had to promise Reuters the data would not be made available on a public website or given to anyone else.} Please treat this data with care, do not posted to github, etc.

\subsection{Testing}

Test your code using the following command line (with your {\tt tfidf.py} is in the same directory):

\begin{alltt}
$ python -m pytest test_tfidf.py
============================= test session starts ==============================
platform darwin -- Python 2.7.7 -- py-1.4.20 -- pytest-2.5.2
collected 5 items 

test_tfidf.py .....

=========================== 5 passed in 0.04 seconds ===========================
\end{alltt}

\noindent If you don't see all tests passing, and there is a problem at a basic level with your software.

Note that the test file imports your file with:

\begin{pyverbatim}
from tfidf import *
\end{pyverbatim}

If you name it incorrectly, the program won't work. 

To test the entire corpus of Reuters articles, also run your program as follows, potentially with a different path specification.
 
\begin{lstlisting}[style=BashInputStyle]
$ python test_corpus.py '../data/reuters-vol1-disk1/*.xml'
\end{lstlisting}

\noindent Note that the quotations around the path specification are required to prevent the command line from expanding {\tt *.xml}. You want that path specification to go in as a single argument, not a list of files.

The core of {\tt test\_corpus.py} is:

\begin{pyverbatim}
(file_to_histo, word_to_numdocs) = create_indexes(files)
for f in files:
    pairs = doc_tfidf(file_to_histo[f], word_to_numdocs, N)
    # convert map to a Counter object so we can use most_common()
    term_pair = Counter(tfmap).most_common(1)[0]
    print os.path.basename(f), "(\%s, \%1.4f)" \% (term_pair[0], term_pair[1])
\end{pyverbatim}

\noindent The output, which I have provided in file {\tt corpus\_output.txt.7z}, starts with (there are actually more files than 81880, but they are mysteriously empty):

\begin{alltt}
81880 files
131674newsML.xml (ewe, 0.7714)
131675newsML.xml (tisa, 0.2909)
131676newsML.xml (ingenico, 0.4040)
131677newsML.xml (lisbon, 0.1876)
131678newsML.xml (drachmas, 0.0844)
131679newsML.xml (satisfying, 0.1774)
13167newsML.xml (cents, 0.3350)
131680newsML.xml (tightness, 0.0891)
131681newsML.xml (tisa, 0.3626)
131682newsML.xml (intervention, 0.1766)
131683newsML.xml (nordic, 0.1249)
131684newsML.xml (oilseeds, 0.1030)
131685newsML.xml (crowns, 0.1299)
131686newsML.xml (trelleborg, 0.2399)
131687newsML.xml (nni, 0.4351)
131688newsML.xml (nantes, 0.3898)
131689newsML.xml (advances, 0.4745)
13168newsML.xml (utilicorp, 0.3165)
131690newsML.xml (sas, 0.2201)
131691newsML.xml (austria, 0.0869)
131692newsML.xml (wage, 0.1244)
131693newsML.xml (herzog, 0.2330)
...
\end{alltt}

My implementation takes about 1 minute 30 seconds to compute TFIDF scores for 81880 XML files loaded from an SSD on a fast machine.  Loading those files takes just 5 seconds.
 
\section{Resources}

I provide for you the following files:

\begin{itemize}
\item {\tt test\_tfidf.py}: some simple tests using py.test.
\item {\tt test\_corpus.py}: prints out the file, term, and TFIDF score for the highest scoring term in each file.
\item {\tt corpus\_output.txt.7z}: compressed output from running test\_corpus.py
\item {\tt reuters-vol1-disk1-subset.7z}: compressed directory full of XML files---the corpus. It is 385M when uncompressed. This file is in Canvas' files area: \href{https://usfca.instructure.com/courses/1553155/files/}{https://usfca.instructure.com/courses/1553155/files/}.
\end{itemize}

\begin{callout}{\bcplume}
{\bf Deliverables}.   {\tt tfidf/tfidf.py} file. Please make sure that there is no extraneous output generated by your code. Tag when completed with {\tt tfidf}.
\end{callout}

\section{Extra credit --- Search Engine}

In this project, we created an index from term to the number of documents that contain that term. If we extend that to be an index from term to the list of documents containing the term, we can get the same results as we did before. The benefit would be that we could also create a search engine.

Given a query such as ``consumer confidence,'' we could merge the list of files containing those two terms and display those to a user. It's fast and works great! The only problem is that we might get 1000 documents back and we'd really like to show the most relevant documents first.  Using the $tf\_map$ index, we can compute a relevance score for a query, relative to a document, by summing the TFIDF scores for each term in the query that is present in the document. The document with the highest two TFIDF scores for ``consumer confidence,'' would be the first document we displayed.

You need to modify the $df$ map from above, use additive smoothing to handle unknown words, and then implement the following function.

\begin{pyverbatim}
def search(query): # query is a string with a list of words
    docs = []
    # find list of documents for each term in query
    # docs = intersection of these files
    # compute sum of TFIDF scores for each term in query relative to each document in docs
    # sort documents by reverse score
    # Returns a list of document filenames in reverse TFIDF order
    return docs
\end{pyverbatim}

\end{fullwidth}

